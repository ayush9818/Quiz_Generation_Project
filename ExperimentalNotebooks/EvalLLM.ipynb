{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70f3e38-793e-4e7c-a777-ce0b711e2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, GenerationConfig\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory,ConversationSummaryMemory, ConversationBufferWindowMemory\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser, CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9c5136-f546-45db-8bb4-3bdce231c6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662cb97749cc4c46847b37d53a7395ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad50527e-475e-47b6-a2ec-5736ded6fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer,\n",
    "                device=5, \n",
    "                max_new_tokens=2000,\n",
    "                do_sample=True, \n",
    "                top_k=20, \n",
    "                top_p=0.7,\n",
    "                early_stopping=True,\n",
    "                num_beams=2\n",
    "               )\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d3f1e8-527e-4290-bcbb-53e300115e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_response_schemas():\n",
    "    \"\"\"Initialises response schemas for StructuredOutputParser\"\"\"\n",
    "    answer_schema = ResponseSchema(name='Answer', description=\"Correct Answer to the given Question\")\n",
    "    explanation_schema = ResponseSchema(name='Explanation', description = 'Explanation why a particular choice is selected as the answer')\n",
    "    \n",
    "    response_schemas = [\n",
    "                        answer_schema,\n",
    "                        explanation_schema]\n",
    "    return response_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8179095d-da7c-4c93-a711-9f6d690c5277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"Answer\": string  // Correct Answer to the given Question\n",
      "\t\"Explanation\": string  // Explanation why a particular choice is selected as the answer\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response_schemas = initialize_response_schemas()\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "791a8e9c-f2e0-4e67-8cf3-b50b74487547",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Role: You are an expert grader. You can evaluate the Quizzes' answers. Given MCQ Quiz Question along with the answer,\n",
    "you need to check the answer and provide the correct choice of answer if it is wrong.\n",
    "\n",
    "User: Below is the input MCQ question with 4 choices and a user-given answer with Explanation. \n",
    "Evalaute the answer and give correct choice if answer is wrong. You should not give your chain of though in the response.\n",
    "Just give the final correct choice of answer along with a brief explanation.\n",
    "\n",
    "Input: \n",
    "Question: {Question} \n",
    "Choice1 : {Choice1}\n",
    "Choice2 : {Choice2}\n",
    "Choice3 : {Choice3}\n",
    "Choice4 : {Choice4}\n",
    "Answer  : {Answer}\n",
    "Explanation : {Explanation}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d32b2d2-5a96-4583-b439-ea994691f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"\"\"Role: This is different Template for example\n",
    "\n",
    "User: Below is the input MCQ question with 4 choices and a user-given answer with Explanation. \n",
    "Evalaute the answer and give correct choice if answer is wrong. You should not give your chain of though in the response.\n",
    "Just give the final correct choice of answer along with a brief explanation.\n",
    "\n",
    "Input: \n",
    "Question: {Question} \n",
    "Choice1 : {Choice1}\n",
    "Choice2 : {Choice2}\n",
    "Choice3 : {Choice3}\n",
    "Choice4 : {Choice4}\n",
    "Answer  : {Answer}\n",
    "Explanation : {Explanation}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4483af5-f25c-44ae-9e94-da238ca4a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt2 = PromptTemplate.from_template(template2)\n",
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"Question\", \"Choice1\", \"Choice2\", \"Choice3\",\"Choice4\",\"Answer\",\"Explanation\",\n",
    "#                      \"format_instructions\"\n",
    "#                     ], template=template\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "681c760f-b460-454c-92e1-cab714c9b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quiz(quiz_path):\n",
    "    with open(quiz_path,'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    data = data.split('\\n\\n')\n",
    "\n",
    "\n",
    "    ans_list = []\n",
    "    for ques in data[1:]:\n",
    "        try:\n",
    "            _ = ques.split('\\n')\n",
    "            question = _[0].split(':')[1]\n",
    "            choice1 = _[1].split(':')[1]\n",
    "            choice2 = _[2].split(':')[1]\n",
    "            choice3 = _[3].split(':')[1]\n",
    "            choice4 = _[4].split(':')[1]\n",
    "            answer = _[5].split(':')[1]\n",
    "            exp = _[6].split(':')[1]\n",
    "    \n",
    "            ques_data = {\n",
    "                \"Question\" : question,\n",
    "                \"Choice1\" : choice1,\n",
    "                \"Choice2\" : choice2,\n",
    "                \"Choice3\" : choice3,\n",
    "                \"Choice4\" : choice4,\n",
    "                \"Answer\" : answer,\n",
    "                \"Explanation\" : exp\n",
    "            }\n",
    "            ans_list.append(ques_data)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return ans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d983252-c632-406d-8969-80bc01bf94eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'In an A/B test, two versions of a website are compared to determine which one converts better. Version A has a conversion rate of 12% and a bounce rate of 40%. Version B has a conversion rate of 15% and a bounce rate of 35%. Based on these statistics, which version of the website is more likely to have a lower average session duration?',\n",
       " 'Choice1': 'Version A',\n",
       " 'Choice2': 'Version B',\n",
       " 'Choice3': 'Both versions have the same average session duration',\n",
       " 'Choice4': 'Insufficient data to determine',\n",
       " 'Answer': 'Choice3',\n",
       " 'Explanation': 'Choice3'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_data = load_quiz('./quizzes/quiz_12.txt')\n",
    "quiz_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893c0248-7d6a-49cd-82ad-0fed7ae2dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "                    llm=hf,\n",
    "                    prompt=prompt, \n",
    "                    output_parser = output_parser\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "789b9812-ef1b-447d-a070-dedb86db53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain2 = LLMChain(\n",
    "                    llm=hf,\n",
    "                    prompt=prompt2, \n",
    "                    output_parser = output_parser\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "373ffb00-993b-4ad8-9238-fed26d483a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = quiz_data[1]\n",
    "data['format_instructions'] = format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b34eddc8-4583-40c4-a137-76296f6d3ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aagarwal/miniconda3/envs/OpenShape/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = llm_chain.invoke(data).get('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2761c019-29c5-48ab-894c-6aed9e39afe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'Choice3', 'Explanation': 'The question does not provide enough data to determine the average session duration for both versions.'}\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb5b55be-23eb-4ccc-9b12-d0735a20080d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aagarwal/miniconda3/envs/OpenShape/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'Choice3', 'Explanation': 'Both versions have the same explanation here'}\n"
     ]
    }
   ],
   "source": [
    "out = llm_chain.invoke(data).get('text')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14ceaeff-c2a5-443e-a54e-cd32b316e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aagarwal/miniconda3/envs/OpenShape/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'Choice3', 'Explanation': 'Both versions have the same explanation needed for this question'}\n"
     ]
    }
   ],
   "source": [
    "out = llm_chain.invoke(data).get('text')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43cea947-aa1d-4214-9e77-945329368c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aagarwal/miniconda3/envs/OpenShape/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'Choice3', 'Explanation': 'Both versions have the same explanation here'}\n"
     ]
    }
   ],
   "source": [
    "out = llm_chain.invoke(data).get('text')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a13d1a-c35a-4167-b0d9-96bbecc0191a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99bd47-8a4d-4459-b96e-2215fe900f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-OpenShape]",
   "language": "python",
   "name": "conda-env-miniconda3-OpenShape-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
