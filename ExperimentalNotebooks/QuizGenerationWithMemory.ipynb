{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "85aad879-3ce3-4cfe-8888-1cac03a45fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, GenerationConfig\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory,ConversationSummaryMemory, ConversationBufferWindowMemory\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser, CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6feac55f-be53-4e39-83e6-4f8b0536c4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f6c9169d6348f1bfd11bfff160d222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f4c9732-1f03-446d-a78c-abb62b6cf7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", \n",
    "                model=model, \n",
    "                tokenizer=tokenizer,\n",
    "                device=0, \n",
    "                max_new_tokens=2000,\n",
    "                do_sample=True, \n",
    "                top_k=20, \n",
    "                top_p=0.7,\n",
    "                early_stopping=True,\n",
    "                num_beams=2\n",
    "               )\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53157255-83cc-4d8c-822c-4112a5ec8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_schema = ResponseSchema(name='Question', description=\"Question Generated on the given topic\")\n",
    "choice1_schema = ResponseSchema(name='Choice1', description='Choice 1 for the given question')\n",
    "choice2_schema = ResponseSchema(name='Choice2', description='Choice 2 for the given question')\n",
    "choice3_schema = ResponseSchema(name='Choice3', description='Choice 3 for the given question')\n",
    "choice4_schema = ResponseSchema(name='Choice4', description='Choice 4 for the given question')\n",
    "answer_schema = ResponseSchema(name='Answer', description='One of the selected choices out of 4 choices given as the answer. Eg Choice1')\n",
    "explanation_schema = ResponseSchema(name='Explanation', description = 'Explanation why a particular choice is selected as the answer')\n",
    "\n",
    "response_schemas = [question_schema, \n",
    "                    choice1_schema,\n",
    "                    choice2_schema,\n",
    "                    choice3_schema,\n",
    "                    choice4_schema,\n",
    "                    answer_schema,\n",
    "                    explanation_schema\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "79b55595-f76d-40c3-86f8-5ce76b427f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "# focused on the topic of {subject}\n",
    "#{chat_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cccf188f-1075-4f5f-8132-80b56b3f60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a KnowledgeStore which generates MCQ questions of desired difficulty on a given topic.\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Human: Generate a MCQ type question for graduate students of {difficulty} difficulty level focussed on {subject}. \n",
    "The question should present a significant challenge, aligning with master's level coursework, and should ideally incorporate real-world applications or data to contextualize the mathematical or statistical concepts involved. \n",
    "Ensure the question demand a deep understanding and application of theoretical principles, possibly involving multiple steps or the integration of several concepts.\n",
    "Ensure that Question is strictly multiple choice question with exactly 4 choices.\n",
    "Choice1, Choice2, Choice3, Choice4\n",
    "The answer should definitely be one of the Choices.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "#\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \n",
    "                     \"difficulty\",\n",
    "                     \"subject\",\n",
    "                     \"format_instructions\"], template=template\n",
    ")\n",
    "#memory = ConversationSummaryMemory(llm=hf, input_key='subject')\n",
    "memory = ConversationBufferWindowMemory(k=5, memory_key=\"chat_history\", input_key='subject')\n",
    "#memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key='subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e9751c00-db55-43b8-b15f-43df865fa65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2b47b1fa-563e-40a1-95fe-f296d2146a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=hf,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "23745b21-e809-43f7-8dd8-9f5c785bc3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aagarwal/miniconda3/envs/OpenShape/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a KnowledgeStore which generates MCQ questions of desired difficulty on a given topic.\n",
      "\n",
      "\n",
      "\n",
      "Human: Generate a MCQ type question for graduate students of easy difficulty level focussed on history of USA. \n",
      "The question should present a significant challenge, aligning with master's level coursework, and should ideally incorporate real-world applications or data to contextualize the mathematical or statistical concepts involved. \n",
      "Ensure the question demand a deep understanding and application of theoretical principles, possibly involving multiple steps or the integration of several concepts.\n",
      "Ensure that Question is strictly multiple choice question with exactly 4 choices.\n",
      "Choice1, Choice2, Choice3, Choice4\n",
      "The answer should definitely be one of the Choices.\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"Question\": string  // Question Generated on the given topic\n",
      "\t\"Choice1\": string  // Choice 1 for the given question\n",
      "\t\"Choice2\": string  // Choice 2 for the given question\n",
      "\t\"Choice3\": string  // Choice 3 for the given question\n",
      "\t\"Choice4\": string  // Choice 4 for the given question\n",
      "\t\"Answer\": string  // One of the selected choices out of 4 choices given as the answer. Eg Choice1\n",
      "\t\"Explanation\": string  // Explanation why a particular choice is selected as the answer\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "out = llm_chain.run({'difficulty' : 'easy',\n",
    "                     'subject' : 'history of USA',\n",
    "                     'format_instructions' : format_instructions\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cb716979-1396-48bf-a8af-ff8a1277e72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'In what year did the first transcontinental railroad in the United States of America get completed?',\n",
       " 'Choice1': '1860',\n",
       " 'Choice2': '1865',\n",
       " 'Choice3': '1870',\n",
       " 'Choice4': '1875',\n",
       " 'Answer': 'Choice2',\n",
       " 'Explanation': 'The first transcontinental railroad in the United States of America was completed in 1865. The Central Pacific Railroad and the Union Pacific Railroad joined their tracks in Promontory Summit, Utah, on May 10, 1869. However, the last spike was not driven until May 10, 1869. Therefore, the correct answer is 1865.'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9866472a-3a81-41b0-ae24-8a0d7f0f0101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'Which event marked the turning point in the American Revolution?',\n",
       " 'Choice1': 'The signing of the Declaration of Independence',\n",
       " 'Choice2': 'The Battle of Saratoga',\n",
       " 'Choice3': 'The Boston Tea Party',\n",
       " 'Choice4': 'The Battle of Bunker Hill',\n",
       " 'Answer': 'The Battle of Saratoga',\n",
       " 'Explanation': 'The Battle of Saratoga, fought in 1777, was a significant turning point in the American Revolution. It was the first major American victory and boosted morale and international support for the American cause.'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cd9e0940-72e0-470f-949e-414e584209cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = llm_chain.run({'human_input' : 'Question on Indian History', 'format_instructions' : format_instructions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "84087902-dd6a-43fb-bbe1-0d04d0c34894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'Which event marked the beginning of the American Revolution?',\n",
       " 'Choice1': 'The Boston Tea Party',\n",
       " 'Choice2': 'The signing of the Declaration of Independence',\n",
       " 'Choice3': 'The Battle of Bunker Hill',\n",
       " 'Choice4': 'The Stamp Act',\n",
       " 'Answer': 'The Boston Tea Party',\n",
       " 'Explanation': \"The Boston Tea Party was a significant political protest by the American colonists in response to the British government's imposition of a tax on tea without the consent of the colonists. This event is considered the beginning of the American Revolution.\"}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c5fc73c-f98e-4d4b-a756-47bf7d0cad41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': \"Which Sultan and in which period established the Delhi Sultanate's first university, modeling it after Al-Jamiat al-Qarawiyyin of Fez, a significant center for the translation and dissemination of Islamic and Persian knowledge in India?\",\n",
       " 'Choice1': 'Sultan Alauddin Khilji (1296-1316)',\n",
       " 'Choice2': 'Sultan Muhammad bin Tughlaq (1325-1351)',\n",
       " 'Choice3': 'Sultan Iltutmish (1211-1236)',\n",
       " 'Choice4': 'Sultan Qutb-ud-din Aibak (1206-1210)',\n",
       " 'Answer': 'Choice2',\n",
       " 'Explanation': \"Sultan Muhammad bin Tughlaq established the Delhi Sultanate's first university in the early 14th century, modeling it after Al-Jamiat al-Qarawiyyin of Fez. This university played a crucial role in the dissemination of Islamic and Persian knowledge in India during the medieval period.\"}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f90e8142-03ba-47df-992a-40d2ef262173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'Which Sultan initiated the construction of the Qutub Minar in Indian history and in which period did this occur? Additionally, explain the significance of the Qutub Minar in the context of Islamic architecture and its influence on future structures.',\n",
       " 'Choice1': 'Mughal Emperor Akbar (1556-1605)',\n",
       " 'Choice2': 'Mughal Emperor Shah Jahan (1628-1658)',\n",
       " 'Choice3': 'Sultan Qutb-ud-din Aibak (1206-1210)',\n",
       " 'Choice4': 'Sultan Alauddin Khilji (1296-1316)',\n",
       " 'Answer': 'Choice3',\n",
       " 'Explanation': \"Sultan Qutb-ud-din Aibak initiated the construction of the Qutub Minar in the late 12th century during the Slave Dynasty. The Qutub Minar is a significant example of early Islamic architecture in India and is considered a precursor to the Indo-Islamic architectural style. It is a masterpiece of brickwork, showcasing intricate carvings and calligraphy. The minaret's influence can be seen in later structures such as the Tomb of Iltutmish and the Alai Darwaza.\"}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "547086b1-9e6e-4967-9893-fb27af65024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry for the misunderstanding earlier. Based on your requirements, here's an example of an easy question on Indian History for graduate students:\\n\\nQuestion: Which Indian ruler, known for his patronage of arts and sciences, is believed to have commissioned the construction of the Qutub Minar in Delhi?\\n\\nA) Akbar\\nB) Shah Jahan\\nC) Muhammad bin Tughlaq\\nD) Qutub-ud-din Aibak\\n\\nAnswer: D) Qutub-ud-din Aibak\\n\\nThis question tests the student's understanding of Indian history and requires them to recall specific details about a historical figure and their contributions. It is a multiple choice question, and the answer is one of the choices provided.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Easy difficulty level question on Indian History\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3697c41c-ccac-4dfa-8493-024b84f59fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm an AI designed to generate questions based on a given topic and difficulty level. However, your request seems to have some contradictions. You've asked for an easy question, but then you've specified that it should align with master's level coursework, involve real-world applications, and require a deep understanding and application of theoretical principles. These requirements are typically associated with more complex, challenging questions.\\n\\nFor an easy question on Indian History for graduate students, I would suggest the following:\\n\\nQuestion: Which Indian ruler, known for his patronage of arts and sciences, is believed to have commissioned the construction of the Qutub Minar in Delhi?\\n\\nA) Akbar\\nB) Shah Jahan\\nC) Muhammad bin Tughlaq\\nD) Qutub-ud-din Aibak\\n\\nAnswer: D) Qutub-ud-din Aibak\\n\\nThis question tests the student's understanding of Indian history and requires them to recall specific details about a historical figure and their contributions. It is a multiple choice question, and the answer is one of the choices provided.\\n\\nThis question does not present a significant challenge, align with master's level coursework, or require a deep understanding and application of theoretical principles. However, it does meet the requirement of being a multiple choice question with the answer being one of the choices provided.\\n\\nIf you meant to ask for a more challenging question, please let me know and I'll be happy to help generate one for you.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.predict(human_input=\"Easy difficulty level question on Indian History\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68c47518-6397-4a8c-81c7-c708b09b5187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aagarwal/miniconda3/envs/OpenShape/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One input key expected got ['subject', 'format_instructions', 'difficulty']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifficulty\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124measy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHistory of India\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_instructions\u001b[39m\u001b[38;5;124m\"\u001b[39m : format_instructions}\n\u001b[0;32m----> 5\u001b[0m \u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenShape/lib/python3.9/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenShape/lib/python3.9/site-packages/langchain/chains/base.py:158\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenShape/lib/python3.9/site-packages/langchain/chains/base.py:455\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenShape/lib/python3.9/site-packages/langchain/memory/chat_memory.py:38\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     input_str, output_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_messages(\n\u001b[1;32m     40\u001b[0m         [HumanMessage(content\u001b[38;5;241m=\u001b[39minput_str), AIMessage(content\u001b[38;5;241m=\u001b[39moutput_str)]\n\u001b[1;32m     41\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenShape/lib/python3.9/site-packages/langchain/memory/chat_memory.py:25\u001b[0m, in \u001b[0;36mBaseChatMemory._get_input_output\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_input_output\u001b[39m(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]\n\u001b[1;32m     23\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         prompt_input_key \u001b[38;5;241m=\u001b[39m \u001b[43mget_prompt_input_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         prompt_input_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenShape/lib/python3.9/site-packages/langchain/memory/utils.py:19\u001b[0m, in \u001b[0;36mget_prompt_input_key\u001b[0;34m(inputs, memory_variables)\u001b[0m\n\u001b[1;32m     17\u001b[0m prompt_input_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(inputs)\u001b[38;5;241m.\u001b[39mdifference(memory_variables \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_input_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne input key expected got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_input_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_input_keys[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: One input key expected got ['subject', 'format_instructions', 'difficulty']"
     ]
    }
   ],
   "source": [
    "input_dict = {\"difficulty\": 'easy',\n",
    "                \"subject\": 'History of India',\n",
    "                \"format_instructions\" : format_instructions}\n",
    "\n",
    "llm_chain.invoke(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29534c4-58c9-4fc0-8508-c84170c2f584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-OpenShape]",
   "language": "python",
   "name": "conda-env-miniconda3-OpenShape-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
